{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introductory example to ensemble models\n",
    "\n",
    "This first notebook aims at emphasizing the benefit of ensemble methods over\n",
    "simple models (e.g. decision tree, linear model, etc.). Combining simple\n",
    "models result in more powerful and robust models with less hassle.\n",
    "\n",
    "We will start by loading the california housing dataset. We recall that the\n",
    "goal in this dataset is to predict the median house value in some district\n",
    "in California based on demographic and geographic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">If you want a deeper overview regarding this dataset, you can refer to the\n",
    "Appendix - Datasets description section at the end of this MOOC.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       " 0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       " 1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       " 2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       " 3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       " 4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       " ...       ...       ...       ...        ...         ...       ...       ...   \n",
       " 20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
       " 20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
       " 20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
       " 20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
       " 20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
       " \n",
       "        Longitude  \n",
       " 0        -122.23  \n",
       " 1        -122.22  \n",
       " 2        -122.24  \n",
       " 3        -122.25  \n",
       " 4        -122.25  \n",
       " ...          ...  \n",
       " 20635    -121.09  \n",
       " 20636    -121.21  \n",
       " 20637    -121.22  \n",
       " 20638    -121.32  \n",
       " 20639    -121.24  \n",
       " \n",
       " [20640 rows x 8 columns],\n",
       " 0        452.6\n",
       " 1        358.5\n",
       " 2        352.1\n",
       " 3        341.3\n",
       " 4        342.2\n",
       "          ...  \n",
       " 20635     78.1\n",
       " 20636     77.1\n",
       " 20637     92.3\n",
       " 20638     84.7\n",
       " 20639     89.4\n",
       " Name: MedHouseVal, Length: 20640, dtype: float64)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "data, target = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "target *= 100  # rescale the target in k$\n",
    "data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check the statistical performance of decision tree regressor with\n",
    "default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 score obtained by cross-validation: 0.354 +/- 0.087\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "cv_results = cross_validate(tree, data, target, n_jobs=2)\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain fair results. However, as we previously presented in the \"tree in\n",
    "depth\" notebook, this model needs to be tuned to overcome over- or\n",
    "under-fitting. Indeed, the default parameters will not necessarily lead to an\n",
    "optimal decision tree. Instead of using the default value, we should search\n",
    "via cross-validation the optimal value of the important parameters such as\n",
    "`max_depth`, `min_samples_split`, or `min_samples_leaf`.\n",
    "\n",
    "We recall that we need to tune these parameters, as decision trees tend to\n",
    "overfit the training data if we grow deep trees, but there are no rules on\n",
    "what each parameter should be set to. Thus, not making a search could lead us\n",
    "to have an underfitted or overfitted model.\n",
    "\n",
    "Now, we make a grid-search to tune the hyperparameters that we mentioned\n",
    "earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 score obtained by cross-validation: 0.523 +/- 0.107\nWall time: 13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [5, 8, None],\n",
    "    \"min_samples_split\": [2, 10, 30, 50],\n",
    "    \"min_samples_leaf\": [0.01, 0.05, 0.1, 1]}\n",
    "cv = 3\n",
    "\n",
    "tree = GridSearchCV(DecisionTreeRegressor(random_state=0),\n",
    "                    param_grid=param_grid, cv=cv, n_jobs=2)\n",
    "cv_results = cross_validate(tree, data, target, n_jobs=2,\n",
    "                            return_estimator=True)\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that optimizing the hyperparameters will have a positive effect\n",
    "on the statistical performance. However, it comes with a higher computational\n",
    "cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a dataframe storing the important information collected during\n",
    "the tuning of the parameters and investigate the results.\n",
    "\n",
    "Now we will use an ensemble method called bagging. More details about this\n",
    "method will be discussed in the next section. In short, this method will use\n",
    "a base regressor (i.e. decision tree regressors) and will train several of\n",
    "them on a slightly modified version of the training set. Then, the\n",
    "predictions of all these base regressors will be combined by averaging.\n",
    "\n",
    "Here, we will use 50 decision trees and check the fitting time as well as the\n",
    "statistical performance on the left-out testing data. It is important to note\n",
    "that we are not going to tune any parameter of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "R2 score obtained by cross-validation: 0.642 +/- 0.083\nWall time: 7.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "base_estimator = DecisionTreeRegressor(random_state=0)\n",
    "bagging_regressor = BaggingRegressor(\n",
    "    base_estimator=base_estimator, n_estimators=20, random_state=0)\n",
    "\n",
    "cv_results = cross_validate(bagging_regressor, data, target, n_jobs=2)\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(f\"R2 score obtained by cross-validation: \"\n",
    "      f\"{scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without searching for optimal hyperparameters, the overall statistical\n",
    "performance of the bagging regressor is better than a single decision tree.\n",
    "In addition, the computational cost is reduced in comparison of seeking\n",
    "for the optimal hyperparameters.\n",
    "\n",
    "This shows the motivation behind the use of an ensemble learner: it gives a\n",
    "relatively good baseline with decent statistical performance without any\n",
    "parameter tuning.\n",
    "\n",
    "Now, we will discuss in detail two ensemble families: bagging and\n",
    "boosting:\n",
    "\n",
    "* ensemble using bootstrap (e.g. bagging and random-forest);\n",
    "* ensemble using boosting (e.g. adaptive boosting and gradient-boosting\n",
    "  decision tree)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('Python39')"
  },
  "interpreter": {
   "hash": "effe5469eba6e2b1ac8a7bf67c428998281de374b9fe83a2f672f89c7077779b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}